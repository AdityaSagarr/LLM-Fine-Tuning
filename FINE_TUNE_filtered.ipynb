{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MA5ewF0uyTsQ",
        "vhC-xTi4yjDQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LLM FINE TUNING USING LORA AND QLORA"
      ],
      "metadata": {
        "id": "lKDzvQj5HqYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Install Required Packages AND Configuration setup"
      ],
      "metadata": {
        "id": "MA5ewF0uyTsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Packages\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "\n",
        "# Import Libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Configuration\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "new_model = \"Llama-2-7b-chat-finetune\"\n",
        "\n",
        "\n",
        "# Load Dataset with Train and Validation Splits\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "dataset = dataset.train_test_split(test_size=0.1)  # 10% for evaluation\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['test']\n"
      ],
      "metadata": {
        "id": "J0nEVKV_PZ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.LoRA and Quantization Parameters AND Tokenizer"
      ],
      "metadata": {
        "id": "vhC-xTi4yjDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA and Quantization Parameters\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "# Load Model and Tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n"
      ],
      "metadata": {
        "id": "kec9qOa5Hrc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.TRAINING"
      ],
      "metadata": {
        "id": "0cIOiieYytOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=25,\n",
        "    save_steps=0,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    evaluation_strategy=\"steps\",  # Evaluate at intervals\n",
        "    eval_steps=25,  # Evaluate every 25 steps\n",
        "    do_eval=True  # Enable evaluation\n",
        ")\n",
        "\n",
        "# Fine-Tuning Setup\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Add evaluation dataset\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments\n",
        ")\n",
        "\n",
        "# Train and Save the Model\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "8844o9lHN915"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Visualize Training Logs with TensorBoard"
      ],
      "metadata": {
        "id": "i3IAuKd5zdjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorBoard for visualization in Colab\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard (this will automatically open the UI in Colab)\n",
        "%tensorboard --logdir ./results"
      ],
      "metadata": {
        "id": "JgAJLEydzjFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}